{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using Base environment\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from pprint import pprint\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "import graspologic.utils as graspologic_utils\n",
    "from graspologic.embed import AdjacencySpectralEmbed\n",
    "\n",
    "from graspologic.datasets import load_drosophila_right\n",
    "from graspologic.plot import heatmap\n",
    "from graspologic.utils import binarize, symmetrize\n",
    "import graspologic.utils as graspologic_utils\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "from graspologic.plot import heatmap\n",
    "from sklearn.manifold import Isomap\n",
    "#import piecewise_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "#from kneed import DataGenerator, KneeLocator\n",
    "\n",
    "def filter_matrix_TC(data, well):\n",
    "\n",
    "    # Extract necessary data from the loaded data structure\n",
    "    adj_matrix = data[well]['win_0']['adj_matrix_predicted']\n",
    "    votes = data[well]['win_0']['votes']  # This variable is loaded but not used in the snippet you provided\n",
    "    corr_peaks = data[well]['win_0']['corr_peaks']\n",
    "    fs = data['config']['data']['fs']  # Sampling frequency\n",
    "\n",
    "    # Initialize a matrix to track synchronization based on correlation peaks\n",
    "    synced_matrix = np.full(adj_matrix.shape, False)\n",
    "    for key in corr_peaks.keys():\n",
    "        if np.all(np.abs(np.array(corr_peaks[key]['delays'])) < 1/fs):\n",
    "            synced_matrix[key[0], key[1]] = True\n",
    "            synced_matrix[key[1], key[0]] = True\n",
    "\n",
    "    # Create the filtered matrix as per the given logic\n",
    "    filtered_matrix = np.logical_and(adj_matrix, np.logical_not(synced_matrix))\n",
    "\n",
    "    return filtered_matrix\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "def find_optimal_neighbors(X):\n",
    "    n_neighbors = 2\n",
    "    while True:\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"error\")\n",
    "                isomap = Isomap(n_neighbors=n_neighbors)\n",
    "                isomap.fit(X)\n",
    "                print(f\"Successful with n_neighbors={n_neighbors}\")\n",
    "                return n_neighbors\n",
    "        except Warning as w:\n",
    "            print(f\"Warning encountered with n_neighbors={n_neighbors}: {w}\")\n",
    "            n_neighbors += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def get_elbows(dat, n=3, threshold=False, plot=True, main=\"\"):\n",
    "    \"\"\"\n",
    "    Given a decreasingly sorted vector, return the given number of elbows.\n",
    "\n",
    "    Args:\n",
    "        dat: an input vector (e.g. a vector of standard deviations) or an input feature matrix.\n",
    "        n: the number of returned elbows.\n",
    "        threshold: either False or a number. If threshold is a number, then all\n",
    "                   the elements in dat that are not larger than the threshold will be ignored.\n",
    "        plot: logical. When True, it depicts a scree plot with highlighted elbows.\n",
    "        main: title for the plot.\n",
    "\n",
    "    Returns:\n",
    "        q: a list of length n containing the positions of the elbows.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(dat, np.ndarray) and len(dat.shape) > 1:\n",
    "        d = np.sort(np.std(dat, axis=0))[::-1]\n",
    "    else:\n",
    "        d = np.sort(dat)[::-1]\n",
    "\n",
    "    if threshold is not False:\n",
    "        d = d[d > threshold]\n",
    "\n",
    "    p = len(d)\n",
    "    if p == 0:\n",
    "        raise ValueError(f\"d must have elements that are larger than the threshold {threshold}!\")\n",
    "\n",
    "    lq = np.zeros(p)  # log likelihood, function of q\n",
    "    for q in range(p):\n",
    "        mu1 = np.mean(d[:q+1])\n",
    "        mu2 = np.mean(d[q+1:]) if q < p-1 else np.nan\n",
    "        sigma2 = (np.sum((d[:q+1] - mu1)**2) + np.sum((d[q+1:] - mu2)**2)) / (p - 1 - (q < p-1))\n",
    "        lq[q] = (np.sum(norm.logpdf(d[:q+1], mu1, np.sqrt(sigma2))) + \n",
    "                 np.sum(norm.logpdf(d[q+1:], mu2, np.sqrt(sigma2))))\n",
    "\n",
    "    q = [np.argmax(lq)]\n",
    "    if n > 1 and q[0] < (p - 1):\n",
    "        q.extend([q[0] + 1 + el for el in get_elbows(d[q[0]+1:], n-1, plot=False)])\n",
    "\n",
    "    if plot:\n",
    "        if isinstance(dat, np.ndarray) and len(dat.shape) > 1:\n",
    "            sdv = d\n",
    "            plt.plot(sdv, marker='o')\n",
    "            plt.xlabel(\"dim\")\n",
    "            plt.ylabel(\"stdev\")\n",
    "            plt.title(main)\n",
    "            plt.scatter(q, sdv[q], s=100, color='red')\n",
    "        else:\n",
    "            plt.plot(dat, marker='o')\n",
    "            plt.title(main)\n",
    "            plt.scatter(q, dat[q], s=100, color='red')\n",
    "        plt.show()\n",
    "\n",
    "    return q\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file paths prepared. 10 time points found.\n",
      "Ready to process 6 wells.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graspologic.embed import AdjacencySpectralEmbed, LaplacianSpectralEmbed\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# NOTE: The functions get_elbows() and filter_matrix_TC() must be defined \n",
    "# and available in your environment for the code to run fully.\n",
    "\n",
    "# ===================================================================\n",
    "# 1. PATH AND CONTROL SETUP\n",
    "# ===================================================================\n",
    "\n",
    "# --- ABSOLUTE PATHS ---\n",
    "SAVE_ROOT_DR = r'/cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/'\n",
    "DATA_ROOT_DR = r'/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/'\n",
    "\n",
    "os.chdir(SAVE_ROOT_DR) \n",
    "\n",
    "# --- METHOD SELECTION CONTROL ---\n",
    "EMBEDDING_METHOD = \"ASE\" # <--- CHANGE THIS TO \"LSE\" WHEN NEEDED\n",
    "\n",
    "# --- WELLS TO PROCESS ---\n",
    "WELLS_TO_PROCESS = [f\"well{i:03d}\" for i in range(6)] # well000 to well005\n",
    "\n",
    "# --- CONSTANTS ---\n",
    "TIME_SERIES_PREFIX_DIV = 'DIV' \n",
    "\n",
    "# --- FOLDER NAMING ---\n",
    "ANALYSIS_TYPE = \"TimeCourse_Embed\"\n",
    "OUTPUT_DIR_NAME = f\"Python_{ANALYSIS_TYPE}_outputs_{EMBEDDING_METHOD}\"\n",
    "\n",
    "OUTPUT_DIR = os.path.join(SAVE_ROOT_DR, OUTPUT_DIR_NAME)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True) \n",
    "\n",
    "\n",
    "# --- EMBEDDING FUNCTION MAPPING ---\n",
    "if EMBEDDING_METHOD == \"LSE\":\n",
    "    EmbeddingClass = LaplacianSpectralEmbed\n",
    "    METHOD_TITLE = \"LSE\"\n",
    "else:\n",
    "    EmbeddingClass = AdjacencySpectralEmbed\n",
    "    METHOD_TITLE = \"ASE\"\n",
    "embedder = EmbeddingClass(n_components=10, check_lcc=False)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 2. FILE PATH MANIPULATION (Runs only once)\n",
    "# ===================================================================\n",
    "\n",
    "# Construct the absolute path for the inner directory structure\n",
    "#dr1_relative = 'Raw Data Week 5.5 to 8.5 (run 8 and LD)/Run 8/'\n",
    "dr1_relative = 'Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/'\n",
    "\n",
    "dr1 = os.path.join(DATA_ROOT_DR, dr1_relative) \n",
    "\n",
    "# --- Extract Prefix for Figure Title ---\n",
    "prefix_parts = dr1_relative.strip('/').split(TIME_SERIES_PREFIX_DIV)\n",
    "TITLE_PREFIX_RAW = prefix_parts[0].strip('/_ ') \n",
    "\n",
    "filenames=os.listdir(dr1)\n",
    "filenames.pop(2)\n",
    "sorted_filenames = sorted(filenames, key=lambda x: int(x.split('DIV ')[1]))\n",
    "\n",
    "# The list of file paths used for loading must be absolute\n",
    "sorted_filenames_updated = [\n",
    "    os.path.join(dr1, filename, 'data.raw_20240521_17h07m.pkl') \n",
    "    for filename in sorted_filenames\n",
    "]\n",
    "\n",
    "# Generate titles: DIV 3, DIV 5, ..., DIV 21\n",
    "div_titles = [f\"DIV {x}\" for x in range(3, 23, 2)]\n",
    "\n",
    "print(f\"Data file paths prepared. {len(sorted_filenames_updated)} time points found.\")\n",
    "print(f\"Ready to process {len(WELLS_TO_PROCESS)} wells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/DIV 3/data.raw_20240521_17h07m.pkl',\n",
       " '/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/DIV 5/data.raw_20240521_17h07m.pkl',\n",
       " '/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/DIV 7/data.raw_20240521_17h07m.pkl',\n",
       " '/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/DIV 9/data.raw_20240521_17h07m.pkl',\n",
       " '/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/DIV 11/data.raw_20240521_17h07m.pkl',\n",
       " '/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/DIV 13/data.raw_20240521_17h07m.pkl',\n",
       " '/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/DIV 15/data.raw_20240521_17h07m.pkl',\n",
       " '/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/DIV 17/data.raw_20240521_17h07m.pkl',\n",
       " '/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/DIV 19/data.raw_20240521_17h07m.pkl',\n",
       " '/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/DIV 21/data.raw_20240521_17h07m.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_filenames_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_Embed_outputs_ASE/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well000__ASE.pdf\n",
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_Embed_outputs_ASE/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well001__ASE.pdf\n",
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_Embed_outputs_ASE/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well002__ASE.pdf\n",
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_Embed_outputs_ASE/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well003__ASE.pdf\n",
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_Embed_outputs_ASE/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well004__ASE.pdf\n",
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_Embed_outputs_ASE/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well005__ASE.pdf\n",
      "\n",
      "All well figures processed and saved.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 3. MAIN PLOTTING AND SAVING LOOP (Iterates over all wells)\n",
    "# ===================================================================\n",
    "\n",
    "for WELL_NAME in WELLS_TO_PROCESS:\n",
    "    \n",
    "    # --- Data Loading (Runs for the current WELL_NAME) ---\n",
    "    all_adj = []\n",
    "    for file_path in sorted_filenames_updated:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pkl.load(f)\n",
    "        # filter_matrix_TC must be defined\n",
    "        adj = filter_matrix_TC(data, WELL_NAME) \n",
    "        all_adj.append(adj)\n",
    "\n",
    "    # --- Filename and Title Construction ---\n",
    "    # The figure title structure: [Prefix] | [Well] | [Method]\n",
    "    fig_suptitle = f\"{TITLE_PREFIX_RAW} | {WELL_NAME} | {METHOD_TITLE}\"\n",
    "\n",
    "    # The filename is derived from the figure title (cleaned)\n",
    "    full_filename_base = (\n",
    "        fig_suptitle.replace(\"/\", \"_\")\n",
    "        .replace(\" \", \"_\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\"|\", \"\")\n",
    "        + \".pdf\"\n",
    "    )\n",
    "    final_pdf_path = os.path.join(OUTPUT_DIR, full_filename_base)\n",
    "\n",
    "    # Create one large figure for the 2x5 grid\n",
    "    fig = plt.figure(figsize=(20, 8)) \n",
    "    fig.suptitle(fig_suptitle, fontsize=20, fontweight='bold', y=0.98)\n",
    "\n",
    "\n",
    "    # Limit loop to 10 items to fit the 2x5 grid (DIV 3 to 21)\n",
    "    for i in range(min(len(all_adj), 10)):\n",
    "        \n",
    "        adj = all_adj[i]\n",
    "        current_title = f\"{div_titles[i]}\"\n",
    "        \n",
    "        # 1. Symmetrize & Extract LCC\n",
    "        adj_symm = ((adj + adj.T) > 0).astype(int)\n",
    "        g = ig.Graph.Adjacency(adj_symm.tolist(), mode=\"undirected\")\n",
    "        lcc = g.connected_components().giant() \n",
    "        \n",
    "        # 2. Check LCC size\n",
    "        if lcc.vcount() < 11:\n",
    "            ax = fig.add_subplot(2, 5, i + 1)\n",
    "            ax.text(0.5, 0.5, \"LCC < 11\", ha='center', va='center', color='red')\n",
    "            ax.set_title(current_title, fontsize=12)\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "\n",
    "        # 3. Run Embedding (ASE or LSE)\n",
    "        mat_lcc = np.array(lcc.get_adjacency().data)\n",
    "        \n",
    "        try:\n",
    "            Xhat = embedder.fit_transform(mat_lcc)\n",
    "        except ValueError:\n",
    "            ax = fig.add_subplot(2, 5, i + 1)\n",
    "            ax.text(0.5, 0.5, f\"{METHOD_TITLE} Failed\", ha='center', va='center', color='red')\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "\n",
    "        # 4. Determine Dimension\n",
    "        embedding_dim = 3 # Forced to 3\n",
    "\n",
    "        # 5. Dynamic Plotting (2D vs 3D)\n",
    "        if embedding_dim >= 3:\n",
    "            ax = fig.add_subplot(2, 5, i + 1, projection='3d')\n",
    "            ax.scatter(Xhat[:, 0], Xhat[:, 1], Xhat[:, 2], c='blue', alpha=0.6, s=15)\n",
    "            ax.set_xlabel(\"Dim 1\", fontsize=8)\n",
    "            ax.set_ylabel(\"Dim 2\", fontsize=8)\n",
    "            ax.set_zlabel(\"Dim 3\", fontsize=8)\n",
    "        else:\n",
    "            ax = fig.add_subplot(2, 5, i + 1)\n",
    "            \n",
    "            if embedding_dim < 2:\n",
    "                ax.text(0.5, 0.5, f\"Dim < 2 ({embedding_dim})\", color=\"orange\", ha='center')\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                ax.scatter(Xhat[:, 0], Xhat[:, 1], c='blue', alpha=0.6, s=15)\n",
    "                ax.set_xlabel(\"Dimension 1\", fontsize=8)\n",
    "                ax.set_ylabel(\"Dimension 2\", fontsize=8)\n",
    "\n",
    "        ax.set_title(current_title, fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
    "\n",
    "    # 6. --- SAVE FIGURE ---\n",
    "    print(f\"Saving figure to: {final_pdf_path}\")\n",
    "    plt.savefig(final_pdf_path, format='pdf', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"\\nAll well figures processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['spike_amp_thresh', 'channel_numbers', 'channel_spikes_per_sec', 'win_0'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This shows that the data doesn't have window \n",
    "with open(sorted_filenames_updated[0], 'rb') as f:\n",
    "    data = pkl.load(f)\n",
    "data['well000'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': {'source_files': '/aoscluster/moped/data/brain_organoid_spikes/TimeCourseData',\n",
       "  'results': '/aoscluster/moped/data/brain_organoid_spikes/TimeCourseData_ecr_results'},\n",
       " 'data': {'fs': 10000,\n",
       "  'spike_amp_thresh_percentile': 5,\n",
       "  'corr_amp_thresh_percentile': None,\n",
       "  'corr_amp_thresh_std': 1},\n",
       " 'windows': {'win_dur': 'None'},\n",
       " 'super_sel': {'recompute': False,\n",
       "  'adj_threshold': 1.0,\n",
       "  'raster_dur': 0.0005,\n",
       "  'corr_type': 'cc',\n",
       "  'n_corr_peaks_max': 4,\n",
       "  'epsilon': 0.003,\n",
       "  'T_list': [0.02, 0.0175, 0.016],\n",
       "  'sigma_list': [0.0004, 0.00055, 0.0007]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['config']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed CP Stats Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===================================================================\n",
    "# 1. PATH AND CONTROL SETUP\n",
    "# ===================================================================\n",
    "\n",
    "# --- ABSOLUTE PATHS ---\n",
    "# (Adjust these paths as needed for your specific environment)\n",
    "SAVE_ROOT_DR = r'/cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/'\n",
    "DATA_ROOT_DR = r'/cis/project/organoid/2024May28 No window data /OneDrive_1_6-17-2024/TimeCourseData_ecr_results/'\n",
    "\n",
    "try:\n",
    "    os.chdir(SAVE_ROOT_DR)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Root directory {SAVE_ROOT_DR} not found. Running in current directory.\")\n",
    "\n",
    "# --- WELLS TO PROCESS ---\n",
    "WELLS_TO_PROCESS = [f\"well{i:03d}\" for i in range(6)] \n",
    "\n",
    "# --- CONSTANTS ---\n",
    "TIME_SERIES_PREFIX_DIV = 'DIV' \n",
    "\n",
    "# --- FOLDER NAMING ---\n",
    "ANALYSIS_TYPE = \"TimeCourse_DIRECTED_CP_Stats\"\n",
    "OUTPUT_DIR_NAME = f\"Python_{ANALYSIS_TYPE}_outputs\"\n",
    "\n",
    "OUTPUT_DIR = os.path.join(SAVE_ROOT_DR, OUTPUT_DIR_NAME)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True) \n",
    "\n",
    "print(\"Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper function updated with Block Reciprocities.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 2. HELPER FUNCTION: Directed CP Statistics + Reciprocity\n",
    "# ===================================================================\n",
    "\n",
    "def get_directed_clique_density_stats(adj):\n",
    "    \"\"\"\n",
    "    Calculates Core-Periphery statistics for a DIRECTED graph, including\n",
    "    densities and reciprocity (rho) for the whole graph and per block.\n",
    "    \"\"\"\n",
    "    # Ensure binary directed adjacency\n",
    "    adj_bin = (adj > 0).astype(int)\n",
    "    \n",
    "    # Create Directed Graph\n",
    "    g = ig.Graph.Adjacency(adj_bin.tolist(), mode=\"directed\")\n",
    "    \n",
    "    # Coreness (mode='all' for structural coreness)\n",
    "    kc = np.array(g.coreness(mode=\"all\"))\n",
    "    max_k = np.max(kc) if len(kc) > 0 else 0\n",
    "    \n",
    "    core_indices = np.where(kc == max_k)[0]\n",
    "    all_indices = np.arange(adj.shape[0])\n",
    "    periph_indices = np.setdiff1d(all_indices, core_indices)\n",
    "    \n",
    "    n_c, n_p = len(core_indices), len(periph_indices)\n",
    "    \n",
    "    # --- Extract Sub-matrices ---\n",
    "    # Rows are Source, Cols are Target\n",
    "    Acc = adj[np.ix_(core_indices, core_indices)] if n_c > 0 else np.array([])\n",
    "    App = adj[np.ix_(periph_indices, periph_indices)] if n_p > 0 else np.array([])\n",
    "    \n",
    "    # Acp: Core -> Periph\n",
    "    Acp = adj[np.ix_(core_indices, periph_indices)] if (n_c > 0 and n_p > 0) else np.array([])\n",
    "    # Apc: Periph -> Core\n",
    "    Apc = adj[np.ix_(periph_indices, core_indices)] if (n_c > 0 and n_p > 0) else np.array([])\n",
    "\n",
    "    # --- 1. EDGE COUNTS (Sums) ---\n",
    "    edges_CC = Acc.sum() if Acc.size > 0 else 0\n",
    "    edges_PP = App.sum() if App.size > 0 else 0\n",
    "    edges_CP = Acp.sum() if Acp.size > 0 else 0\n",
    "    edges_PC = Apc.sum() if Apc.size > 0 else 0\n",
    "    \n",
    "    total_edges = edges_CC + edges_PP + edges_CP + edges_PC\n",
    "\n",
    "    # --- 2. DENSITIES ---\n",
    "    max_CC = n_c * (n_c - 1)\n",
    "    max_PP = n_p * (n_p - 1)\n",
    "    max_CP = n_c * n_p\n",
    "    max_PC = n_p * n_c # Same as CP\n",
    "    \n",
    "    dens_CC = edges_CC / max_CC if max_CC > 0 else 0\n",
    "    dens_PP = edges_PP / max_PP if max_PP > 0 else 0\n",
    "    dens_CP = edges_CP / max_CP if max_CP > 0 else 0\n",
    "    dens_PC = edges_PC / max_PC if max_PC > 0 else 0\n",
    "\n",
    "    # --- 3. RECIPROCITY (RHO) CALCULATIONS ---\n",
    "    # Formula: sum(A * A.T) / sum(A)\n",
    "    # This counts *endpoints* of mutual edges. If i<->j exists, it contributes 2 to num, 2 to denom.\n",
    "    \n",
    "    # A. Global Reciprocity\n",
    "    num_rho_global = np.sum(adj * adj.T)\n",
    "    den_rho_global = np.sum(adj)\n",
    "    rho_global = num_rho_global / den_rho_global if den_rho_global > 0 else 0\n",
    "\n",
    "    # B. Block-wise Reciprocities\n",
    "    \n",
    "    # Rho CC: (Mutuals inside Core) / (Edges inside Core)\n",
    "    rho_CC = 0\n",
    "    if Acc.size > 0 and edges_CC > 0:\n",
    "        rho_CC = np.sum(Acc * Acc.T) / edges_CC\n",
    "        \n",
    "    # Rho PP: (Mutuals inside Periph) / (Edges inside Periph)\n",
    "    rho_PP = 0\n",
    "    if App.size > 0 and edges_PP > 0:\n",
    "        rho_PP = np.sum(App * App.T) / edges_PP\n",
    "\n",
    "    # Rho Inter-Block (CP and PC interface)\n",
    "    # Mutuals here are edges where (c->p) AND (p->c) exist.\n",
    "    # Numerator: 2 * count of mutual pairs.\n",
    "    # We can calculate this by interacting Acp and Apc.\n",
    "    # Note: Acp[i,j] corresponds to Core_i -> Periph_j.\n",
    "    #       Apc[j,i] corresponds to Periph_j -> Core_i.\n",
    "    #       So we check element-wise: Acp * (Apc Transposed)\n",
    "    rho_inter = 0\n",
    "    edges_inter = edges_CP + edges_PC\n",
    "    if Acp.size > 0 and edges_inter > 0:\n",
    "        # Acp is (Nc x Np). Apc is (Np x Nc). Apc.T is (Nc x Np).\n",
    "        # Element-wise mult finds where both exist.\n",
    "        mutual_inter_matrix = Acp * Apc.T \n",
    "        # Sum gives count of mutual PAIRS (from perspective of C->P).\n",
    "        # Each pair implies 2 mutual endpoints (one in C, one in P).\n",
    "        # Formula consistency: sum(endpoints) / sum(all edges)\n",
    "        rho_inter = (2 * np.sum(mutual_inter_matrix)) / edges_inter\n",
    "\n",
    "    return {\n",
    "        'dens_CC': dens_CC, 'dens_CP': dens_CP,\n",
    "        'dens_PC': dens_PC, 'dens_PP': dens_PP,\n",
    "        'n_core': n_c, 'n_periph': n_p,\n",
    "        # Reciprocities\n",
    "        'rho_global': rho_global,\n",
    "        'rho_CC': rho_CC,\n",
    "        'rho_PP': rho_PP,\n",
    "        'rho_CP': rho_inter, # CP and PC share the same reciprocity score\n",
    "        'rho_PC': rho_inter\n",
    "    }\n",
    "\n",
    "print(\"Helper function updated with Block Reciprocities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Found 10 files.\n",
      "Ready to process 6 wells: ['well000', 'well001', 'well002', 'well003', 'well004', 'well005']\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 3. FILE PATH PREPARATION\n",
    "# ===================================================================\n",
    "\n",
    "# Note: Adjust relative path as per your specific folder structure\n",
    "#dr1_relative = 'Raw Data Week 5.5 to 8.5 (run 8 and LD)/Run 8/'\n",
    "dr1_relative = 'Raw Data Week 9.5 to 12.5 (run 6 and 7)/Run 7/'\n",
    "dr1 = os.path.join(DATA_ROOT_DR, dr1_relative) \n",
    "\n",
    "# Robust safe-check for file listing\n",
    "if os.path.exists(dr1):\n",
    "    filenames = os.listdir(dr1)\n",
    "    # Replicating your logic of popping the 3rd element if needed (index 2)\n",
    "    if len(filenames) > 2: filenames.pop(2)\n",
    "    \n",
    "    # Sorting logic\n",
    "    try:\n",
    "        sorted_filenames = sorted(filenames, key=lambda x: int(x.split('DIV ')[1]))\n",
    "        \n",
    "        sorted_filenames_updated = [\n",
    "            os.path.join(dr1, f, 'data.raw_20240521_17h07m.pkl') for f in sorted_filenames\n",
    "        ]\n",
    "        \n",
    "        div_titles = [f\"DIV {x}\" for x in range(3, 23, 2)]\n",
    "        \n",
    "        # Extract title prefix\n",
    "        prefix_parts = dr1_relative.strip('/').split(TIME_SERIES_PREFIX_DIV)\n",
    "        TITLE_PREFIX_RAW = prefix_parts[0].strip('/_ ') \n",
    "        \n",
    "        print(f\"Setup complete. Found {len(sorted_filenames_updated)} files.\")\n",
    "        print(f\"Ready to process {len(WELLS_TO_PROCESS)} wells: {WELLS_TO_PROCESS}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during file sorting: {e}\")\n",
    "        sorted_filenames_updated = []\n",
    "        div_titles = []\n",
    "        TITLE_PREFIX_RAW = \"Unknown_Dataset\"\n",
    "else:\n",
    "    print(f\"Directory not found: {dr1}\")\n",
    "    sorted_filenames_updated = []\n",
    "    div_titles = []\n",
    "    TITLE_PREFIX_RAW = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_DIRECTED_CP_Stats_outputs/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well000__DIRECTED_CP_STATS.pdf\n",
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_DIRECTED_CP_Stats_outputs/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well001__DIRECTED_CP_STATS.pdf\n",
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_DIRECTED_CP_Stats_outputs/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well002__DIRECTED_CP_STATS.pdf\n",
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_DIRECTED_CP_Stats_outputs/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well003__DIRECTED_CP_STATS.pdf\n",
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_DIRECTED_CP_Stats_outputs/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well004__DIRECTED_CP_STATS.pdf\n",
      "Saving figure to: /cis/home/tchen94/tianyi/Organoid/Time course LTP(END)/Python_TimeCourse_DIRECTED_CP_Stats_outputs/Raw_Data_Week_9.5_to_12.5_run_6_and_7_Run_7__well005__DIRECTED_CP_STATS.pdf\n",
      "\n",
      "All DIRECTED CP Stats well figures processed successfully.\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 4. MAIN CP STATS PLOTTING AND SAVING LOOP (Revised)\n",
    "# ===================================================================\n",
    "\n",
    "for WELL_NAME in WELLS_TO_PROCESS:\n",
    "    \n",
    "    # --- Load Data for the current well ---\n",
    "    all_adj = []\n",
    "    if not sorted_filenames_updated:\n",
    "        print(\"No files to process.\")\n",
    "        break\n",
    "        \n",
    "    for file_path in sorted_filenames_updated:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pkl.load(f)\n",
    "        # Using existing filter function\n",
    "        adj = filter_matrix_TC(data, WELL_NAME) \n",
    "        all_adj.append(adj)\n",
    "\n",
    "    # --- Title and Filename Logic ---\n",
    "    fig_suptitle = f\"{TITLE_PREFIX_RAW} | {WELL_NAME} | DIRECTED CP_STATS\"\n",
    "    full_filename = (fig_suptitle.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
    "                     .replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\")\n",
    "                     .replace(\"|\", \"\") + \".pdf\")\n",
    "    final_pdf_path = os.path.join(OUTPUT_DIR, full_filename)\n",
    "\n",
    "    # Setup Figure: 2x5 grid\n",
    "    fig = plt.figure(figsize=(22, 12))\n",
    "    fig.suptitle(fig_suptitle, fontsize=20, fontweight='bold', y=0.98)\n",
    "\n",
    "    for i in range(min(len(all_adj), 10)):\n",
    "        adj = all_adj[i]\n",
    "        current_title = div_titles[i] if i < len(div_titles) else f\"Time {i}\"\n",
    "        ax = fig.add_subplot(2, 5, i + 1)\n",
    "        \n",
    "        # --- 1. Directed Graph Creation ---\n",
    "        adj_bin = (adj > 0).astype(int)\n",
    "        g = ig.Graph.Adjacency(adj_bin.tolist(), mode=\"directed\")\n",
    "        \n",
    "        # --- 2. LCC (Weakly Connected) ---\n",
    "        lcc_g = g.components(mode=\"weak\").giant()\n",
    "        n_original, n_lcc = g.vcount(), lcc_g.vcount()\n",
    "        \n",
    "        # --- 3. Size Threshold ---\n",
    "        if n_lcc < 11:\n",
    "            ax.text(0.5, 0.5, \"LCC size < 11\", color=\"red\", ha='center', va='center', fontsize=12)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(current_title, fontweight='bold')\n",
    "            continue\n",
    "\n",
    "        # --- 4. Calculate Stats ---\n",
    "        mat_lcc = np.array(lcc_g.get_adjacency().data)\n",
    "        \n",
    "        try:\n",
    "            # The function now calculates ALL rhos internally\n",
    "            stats = get_directed_clique_density_stats(mat_lcc)\n",
    "            \n",
    "            # Format Text\n",
    "            text_str = (\n",
    "                \"--- Densities (Dir) ---\\n\"\n",
    "                f\"C -> C: {stats['dens_CC']:.3f}\\n\"\n",
    "                f\"C -> P: {stats['dens_CP']:.3f}\\n\"\n",
    "                f\"P -> C: {stats['dens_PC']:.3f}\\n\"\n",
    "                f\"P -> P: {stats['dens_PP']:.3f}\\n\\n\"\n",
    "                \n",
    "                \"--- Block Symmetry ---\\n\"\n",
    "                f\"Rho CC: {stats['rho_CC']:.3f}\\n\"\n",
    "                f\"Rho CP: {stats['rho_CP']:.3f}\\n\"\n",
    "                f\"Rho PC: {stats['rho_PC']:.3f}\\n\"\n",
    "                f\"Rho PP: {stats['rho_PP']:.3f}\\n\\n\"\n",
    "\n",
    "                \"--- Counts ---\\n\"\n",
    "                f\"Core: {stats['n_core']}\\n\"\n",
    "                f\"Peri: {stats['n_periph']}\\n\"\n",
    "                f\"LCC Size: {n_lcc}\\n\"\n",
    "                f\"Orig Size: {n_original}\\n\\n\"\n",
    "                                \n",
    "                \"--- Global Symmetry ---\\n\"\n",
    "                f\"Mutual/All: {stats['rho_global']:.3f}\\n\\n\"\n",
    "                \n",
    "                \"--- Ratios ---\\n\"\n",
    "                f\"Core/LCC: {stats['n_core']/n_lcc:.3f}\\n\"\n",
    "                f\"Peri/LCC: {stats['n_periph']/n_lcc:.3f}\"\n",
    "            )\n",
    "            ax.text(0.5, 0.5, text_str, ha='center', va='center', family='monospace', fontsize=11)\n",
    "            \n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f\"Error: {str(e)}\", color='red', ha='center')\n",
    "\n",
    "        ax.axis('off')\n",
    "        ax.set_title(current_title, fontweight='bold', fontsize=14)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
    "    \n",
    "    print(f\"Saving figure to: {final_pdf_path}\")\n",
    "    plt.savefig(final_pdf_path, format='pdf', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"\\nAll DIRECTED CP Stats well figures processed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
